{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias a priori\n",
    "\n",
    "*When implementing an AI system, fairness and biases must be an important component during conception, especially when dealing with sensitive information, and/or Personally Identifiable Information (PII), and/or Personal Health Information (PHI). Indeed, not only those information are bound to the law (e.g., GDPR in Europe), but they are also bound to a brand image challenge.*\n",
    "\n",
    "Today's example aims at **assigning a risk with recruitment data**.\n",
    "\n",
    "Before implementing any AI system to predict the likelihood of a candidate to be hired, **AI engineers AND business stakeholders** should:\n",
    "\n",
    "- Sit and identify potential sources of biases\n",
    "- Define one or several metrics that will quantify the bias of the AI system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "StackOverflow's annual user-generated survey (over 70,000 responses from over 180 countries) of developers examines all aspects of the developer experience, from learning code to preferred technologies, version control and work experience.\n",
    "\n",
    "From the survey results, we have built a dataset with the following columns:\n",
    "- **Age**: age of the applicant, >35 years old or <35 years old *(categorical)*\n",
    "- **EdLevel**: education level of the applicant (Undergraduate, Master, PhD...) *(categorical)*\n",
    "- **Gender**: gender of the applicant, (Man, Woman, or NonBinary) *(categorical)*\n",
    "- **MainBranch**: whether the applicant is a profesional developer *(categorical)*\n",
    "- **YearsCode**: how long the applicant has been coding *(integer)*\n",
    "- **YearsCodePro**: how long the applicant has been coding in a professional context, *(integer)*\n",
    "- **PreviousSalary**: the applicant's previous job salary *(float)*\n",
    "- **ComputerSkills**: number of computer skills known by the applicant *(integer)*\n",
    "- **Employed**: target variable, whether the applicant has been hired *(categorical)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness performance\n",
    "\n",
    "To facilitate the development of a responsible model, we use the dalex python package: https://dalex.drwhy.ai/\n",
    "\n",
    "\n",
    "Quoting Dalex' tutorial:\n",
    "\n",
    "\n",
    "> The idea is that ratios between scores of privileged and unprivileged metrics should be close to 1. The closer, the fairer. To relax this criterion a little bit, it can be written more thoughtfully:\n",
    "\n",
    "> $$ \\forall i \\in \\{a, b, ..., z\\}, \\quad \\epsilon < \\frac{metric_i}{metric_{privileged}} < \\frac{1}{\\epsilon}.$$\n",
    "\n",
    "> Where the epsilon is a value between 0 and 1, it should be a minimum acceptable value of the ratio. On default, it is 0.8, which adheres to four-fifths rule (80% rule) often looked at in hiring, for example.\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrics used\n",
    "\n",
    "Description of the metrics used for the fairness performance evaluation of each strategys:\n",
    "\n",
    "\n",
    "\n",
    "- **Equal opportunity ratio** computed from True positive rate (recall)\n",
    "\n",
    "> This number describes the proportions of correctly classified positive instances.\n",
    "\n",
    "> $TPR = \\frac{TP}{P}$\n",
    "\n",
    "- **Predictive parity ratio** computed from Positive predicted value (precision)\n",
    "\n",
    "> This number describes the ratio of samples which were correctly classified as positive from all the positive predictions.\n",
    "\n",
    "> $PPV = \\frac{TP}{TP + FP}$\n",
    "\n",
    "- **Accuracy equality ratio** computed from Accuracy\n",
    "\n",
    "> This number is the ratio of the correctly classified instances (positive and negative) of all decisions.\n",
    "\n",
    "> $ACC = \\frac{TP + TN}{TP + FP + TN + FN}$\n",
    "\n",
    "- **Predictive equality ratio** computed from False positive rate\n",
    "\n",
    "> This number describes the share the proportion of actual negatives which was falsely classified as positive.\n",
    "\n",
    "> $FPR = \\frac{FP}{TP + TN}$\n",
    "\n",
    "- **Statistical parity ratio** computed from Positive rate\n",
    "\n",
    "> This number is the overall rate of positively classified instances, including both correct and incorrect decisions.\n",
    "\n",
    "> $PR = \\frac{TP + FP}{TP + FP + TN + FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "1. Fairness metrics work the exact same way as performance metrics do. If one was to fit a model on the entire dataset and foster overfitting (namely, skipping a `train_test_split` operation), she would end up with a non biased model.\n",
    "2. A lots of metrics can be computed. It is important to define early in the conception which are the critical metrics to monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `model_fairness` returns a fairness object from which fairness evaluations can be conducted. Notice that every metrics inherited from the confusion matrix are computed during the instantiation.\n",
    "\n",
    "Two methods can then be performed:\n",
    "- The `fairness_check` method, which returns a report on the fairness of the model. It requires an epsilon parameter that corresponds to the threshold ratio below which a given metric is considered to be unfair (default value is 0.8).\n",
    "- The `plot` method, which allows to visualize the main fairness ratios between the protected subgroups and the privileged one."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da4dc67900340da4a0087cce73923c73336cfa3732b0a8c639c3a6c2e448da77"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
